{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b0f6b7-8028-4a41-8fd0-b49e6a495983",
   "metadata": {},
   "source": [
    "# Current, Best Approach to Fine-Tuning CLIP\n",
    "\n",
    "This notebook will keep the best, most current approach to fine-tuning the CLIP model with data from Open Context and other archaeological sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda41c98-76ef-48cd-88f5-534054a1f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError, URLError\n",
    "from sklearn.model_selection import train_test_split\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc50852-3a41-427a-ae66-833e0986ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image_convert_to_jpg(uri, folder, uuid, caption, compression_quality=50):\n",
    "    \"\"\"Downloads an image, makes sure it is saved as a jpeg\"\"\"\n",
    "    uuid = str(uuid)\n",
    "    # Remove line breaks from the captions.\n",
    "    caption = caption.replace('\\n', ' ')\n",
    "    new_image_path = os.path.join(folder, f'{uuid}.jpg')\n",
    "    if os.path.exists(new_image_path):\n",
    "        # We already have this so skip\n",
    "        return {\"image_path\": new_image_path, \"caption\": caption}\n",
    "    parse_object = urlparse(uri)\n",
    "    _, ext_from_url = os.path.splitext(parse_object.path)\n",
    "    if isinstance(ext_from_url, bytes):\n",
    "        ext_from_url = ext_from_url.decode(\"utf-8\") \n",
    "    ext_from_url = ext_from_url.lower().replace('.', '')\n",
    "    if ext_from_url in ['jpg', 'jpeg']:\n",
    "        try:\n",
    "            urlretrieve(uri, new_image_path)\n",
    "            return {\"image_path\": new_image_path, \"caption\": caption}\n",
    "        except (HTTPError, URLError) as error:\n",
    "            print(f\"Download error for URL {uri}\", end='\\r')\n",
    "            print(error, end='\\r')\n",
    "            return None\n",
    "    # Not a jpg\n",
    "    download_ok = None\n",
    "    try:\n",
    "        response = requests.get(uri)\n",
    "        response.raise_for_status()\n",
    "        # Check the file type (extension) and convert to JPG if needed\n",
    "        content_type = response.headers['Content-Type']\n",
    "        if content_type.startswith('image/'):\n",
    "            extension = content_type.split('/')[1]\n",
    "            if extension.lower() not in ('jpg', 'jpeg'):\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img.save(new_image_path, 'JPEG', quality=compression_quality)\n",
    "                print(f\"Converted and saved {uri} as JPG: {new_image_path}\", end='\\r')\n",
    "                download_ok = True\n",
    "            else:\n",
    "                with open(new_image_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                with Image.open(new_image_path) as img:\n",
    "                    # Save the image with the desired compression quality\n",
    "                    img.save(new_image_path, format='JPEG', quality=compression_quality)\n",
    "                print(f\"Downloaded and saved {uri} as JPG: {new_image_path}\", end='\\r')\n",
    "                download_ok = True\n",
    "        else:\n",
    "            print(f\"Skipping {uri} - Not an image\", end='\\r')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {uri}: {str(e)}\", end='\\r')\n",
    "        download_ok = False\n",
    "    if not download_ok:\n",
    "        return None\n",
    "    return {\"image_path\": new_image_path, \"caption\": caption}\n",
    "\n",
    "\n",
    "def download_and_rename(row, folder):\n",
    "    \"\"\"Downloads an image file and saves with the media item UUID as the filename\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    uri = row['image_file__uri']\n",
    "    # Check if uri exists and is a string\n",
    "    if uri and isinstance(uri, str):\n",
    "        uuid = row['media__uuid']\n",
    "        caption = row['caption']\n",
    "        return download_image_convert_to_jpg(uri, folder, uuid, caption)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Writing to 'jsonl' files\n",
    "def write_to_jsonl(new_data, jsonl_file):\n",
    "    \"\"\"Makes JSONL file with new_data\"\"\"\n",
    "    with open(jsonl_file, 'w') as file:\n",
    "        for json_dict in new_data:\n",
    "            if not json_dict:\n",
    "                continue\n",
    "            line = json.dumps(json_dict)\n",
    "            file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7af1414-95d2-4b3f-af38-26333a590bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 49200 captioned images, and will allocate 45510 for training and 3690 for testing\n"
     ]
    }
   ],
   "source": [
    "# Get the root_path for this jupyter notebook repo.\n",
    "repo_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "\n",
    "data_path = os.path.join(repo_path, 'json_data', 'artifact_images_w_sentence_captions.json')\n",
    "df = pd.read_json(data_path)\n",
    "\n",
    "# Change these as desired\n",
    "CAPTIONED_IMAGE_COUNT = len(df.index)\n",
    "TRAIN_SIZE = int(round((CAPTIONED_IMAGE_COUNT * .925), 0))\n",
    "TEST_SIZE = CAPTIONED_IMAGE_COUNT - TRAIN_SIZE\n",
    "\n",
    "print(f'We have {CAPTIONED_IMAGE_COUNT} captioned images, and will allocate {TRAIN_SIZE} for training and {TEST_SIZE} for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a2b3b8-f54a-4866-aa8b-2095694c8fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 504: Gateway Time-outiif.archivelab.org/iiif/opencontext-24-19660252bwjpg/full/675,/0/default.jpgpg16.1.97/b2016197-2.jpegClient Error: Not Found for url: https://artiraq.org/static/opencontext/bade-museum/preview/2018-objects/photographs/B2018.1.80/b2018180-4.pg2jpg\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_file = os.path.join(repo_path, 'files', 'train.json')\n",
    "test_data_file = os.path.join(repo_path, 'files', 'test.json')\n",
    "\n",
    "# If we don't have a train_data_file or a test data file, go out and make them!\n",
    "if not os.path.exists(train_data_file) or not os.path.exists(test_data_file):\n",
    "    # Separate out a training dataframe (train_df), a test dataframe (test_df)\n",
    "    train_df, rem_df = train_test_split(df, train_size=TRAIN_SIZE, random_state=42)\n",
    "    test_df = rem_df.sample(TEST_SIZE, random_state=42)\n",
    "    \n",
    "    train_files = [os.path.join(repo_path, 'files', 'training'),]\n",
    "    test_files = [os.path.join(repo_path, 'files', 'testing'),]\n",
    "    \n",
    "    train_data_file = os.path.join(repo_path, 'files', 'train.json')\n",
    "    test_data_file = os.path.join(repo_path, 'files', 'test.json')\n",
    "    \n",
    "    # Process train and test data\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        train_data = list(executor.map(download_and_rename, [row for _, row in train_df.iterrows()], train_files*len(train_df)))\n",
    "        test_data = list(executor.map(download_and_rename, [row for _, row in test_df.iterrows()], test_files*len(test_df)))\n",
    "    \n",
    "    write_to_jsonl(train_data, train_data_file)\n",
    "    write_to_jsonl(test_data, test_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f506e23-39cd-4785-94dc-4d948919d0a0",
   "metadata": {},
   "source": [
    "Now that we have the training and testing data files and the image files, let's train the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a154cfe6-b7e4-43b7-813b-50cdd653ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: datasets in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (2.14.6)\n",
      "Requirement already satisfied: Pillow in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: numpy in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torchvision) (1.26.1)\n",
      "Requirement already satisfied: requests in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchvision) (12.3.52)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.36.0.dev0 requires tokenizers<0.15,>=0.14, but you have tokenizers 0.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (0.4.0)\n",
      "Requirement already satisfied: filelock in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.52)\n",
      "Requirement already satisfied: requests in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision datasets Pillow\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2888efeb-87ed-4432-9cb2-493ae58c094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first image: /home/ekansa/github/archaeology-images-ai/files/training/0f0655bc-ad08-4c9a-9dae-d3273b7f0a22.jpg, caption: 'Image of an archaeological artifact found at Tell en-Nasbeh, in Palestinian Authority. This example of lithics, mainly consists of chert flint (rock). Condition: Good; Category Type: Lithic; Material: Flint; Subcatagory: Lithic - Tool; Completeness: Fragment; Manufacture: Handmade'\n"
     ]
    }
   ],
   "source": [
    "# test loading it back in\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=train_data_file)\n",
    "print(f\"first image: {dataset['train'][0]['image_path']}, caption: '{dataset['train'][0]['caption']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11702f-9bb3-4d4c-a7af-7ee52bddbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/damian0815/finetune-clip-huggingface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77cc460e-93c9-4d50-b72a-172e0a6987ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id =  \"openai/clip-vit-base-patch32\" # this was the clip version for stable diffusion 1.5\n",
    "#repo_id = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\" # this was the clip version for stable diffusion 2.0 onwards\n",
    "# however, using it requires more memory than I have available. More than what's available free tier google colab too.\n",
    "\n",
    "result_output_folder = os.path.join(repo_path, 'results')\n",
    "\n",
    "batch_size = 64\n",
    "num_train_epochs = 20\n",
    "max_token_seq_length = 77 # probably should be 100\n",
    "learning_rate = '5e-5'\n",
    "warmup_steps = 2\n",
    "weight_decay = 0.2\n",
    "# NOTE ON learning_rate = \"1e-4\" # the prior parameter was \"5e-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab05e232-e261-44d9-84d6-fe79ac70a290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning openai/clip-vit-base-patch32 for 20 epochs with batch size 64, and then saving output to /home/ekansa/github/archaeology-images-ai/results.\n",
      "python -W ignore finetune-clip-huggingface/huggingface_finetune_clip.py     --output_dir /home/ekansa/github/archaeology-images-ai/results     --model_name_or_path openai/clip-vit-base-patch32     --train_file /home/ekansa/github/archaeology-images-ai/files/train.json     --validation_file /home/ekansa/github/archaeology-images-ai/files/test.json     --image_column=\"image_path\"     --overwrite_output_dir=True     --max_seq_length=77     --num_train_epochs=20     --caption_column=\"caption\"     --overwrite_cache=True     --remove_unused_columns=False     --do_train=True     --per_device_train_batch_size=64     --per_device_eval_batch_size=64     --learning_rate=\"5e-5\" --warmup_steps=\"2\" --weight_decay 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Finetuning {repo_id} for {num_train_epochs} epochs with batch size {batch_size}, and then saving output to {result_output_folder}.\")\n",
    "\n",
    "print(f\"\"\"python -W ignore finetune-clip-huggingface/huggingface_finetune_clip.py \\\n",
    "    --output_dir {result_output_folder} \\\n",
    "    --model_name_or_path {repo_id} \\\n",
    "    --train_file {train_data_file} \\\n",
    "    --validation_file {test_data_file} \\\n",
    "    --image_column=\"image_path\" \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --max_seq_length={max_token_seq_length} \\\n",
    "    --num_train_epochs={num_train_epochs} \\\n",
    "    --caption_column=\"caption\" \\\n",
    "    --overwrite_cache=True \\\n",
    "    --remove_unused_columns=False \\\n",
    "    --do_train=True \\\n",
    "    --per_device_train_batch_size={batch_size} \\\n",
    "    --per_device_eval_batch_size={batch_size} \\\n",
    "    --learning_rate=\"{learning_rate}\" --warmup_steps=\"{warmup_steps}\" --weight_decay {weight_decay}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523a64d-2f24-461d-b5ab-50f9a4426324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Disable this running in Jupyter. Too many updates to the Web client usually break things.\n",
    "    !python -W ignore finetune-clip-huggingface/huggingface_finetune_clip.py \\\n",
    "        --output_dir {result_output_folder} \\\n",
    "        --model_name_or_path {repo_id} \\\n",
    "        --train_file {train_data_file} \\\n",
    "        --validation_file {test_data_file} \\\n",
    "        --image_column image \\\n",
    "        --overwrite_output_dir=True \\\n",
    "        --max_seq_length={max_token_seq_length} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        --caption_column caption \\\n",
    "        --overwrite_cache=True \\\n",
    "        --remove_unused_columns=False \\\n",
    "        --do_train \\\n",
    "        --per_device_train_batch_size={batch_size} \\\n",
    "        --per_device_eval_batch_size={batch_size} \\\n",
    "        --learning_rate=\"{learning_rate}\" --warmup_steps=\"{warmup_steps}\" --weight_decay {weight_decay}\n",
    "    print(\"--\\nDONE\")\n",
    "    print(f\"If it worked, trained data should be in {result_output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bef16-28bd-4825-8a99-9fcc3d7d1b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193aabc-d1d8-40af-94f6-5a69b7899625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
