{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b0f6b7-8028-4a41-8fd0-b49e6a495983",
   "metadata": {},
   "source": [
    "# Current, Best Approach to Fine-Tuning CLIP\n",
    "\n",
    "This notebook will keep the best, most current approach to fine-tuning the CLIP model with data from Open Context and other archaeological sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda41c98-76ef-48cd-88f5-534054a1f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError, URLError\n",
    "from sklearn.model_selection import train_test_split\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc50852-3a41-427a-ae66-833e0986ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image_convert_to_jpg(uri, folder, uuid, caption, compression_quality=50):\n",
    "    \"\"\"Downloads an image, makes sure it is saved as a jpeg\"\"\"\n",
    "    uuid = str(uuid)\n",
    "    # Remove line breaks from the captions.\n",
    "    caption = caption.replace('\\n', ' ')\n",
    "    new_image_path = os.path.join(folder, f'{uuid}.jpg')\n",
    "    if os.path.exists(new_image_path):\n",
    "        # We already have this so skip\n",
    "        return {\"image\": new_image_path, \"caption\": caption}\n",
    "    parse_object = urlparse(uri)\n",
    "    _, ext_from_url = os.path.splitext(parse_object.path)\n",
    "    if isinstance(ext_from_url, bytes):\n",
    "        ext_from_url = ext_from_url.decode(\"utf-8\") \n",
    "    ext_from_url = ext_from_url.lower().replace('.', '')\n",
    "    if ext_from_url in ['jpg', 'jpeg']:\n",
    "        try:\n",
    "            urlretrieve(uri, new_image_path)\n",
    "            return {\"image\": new_image_path, \"caption\": caption}\n",
    "        except (HTTPError, URLError) as error:\n",
    "            print(f\"Download error for URL {uri}\", end='\\r')\n",
    "            print(error, end='\\r')\n",
    "            return None\n",
    "    # Not a jpg\n",
    "    download_ok = None\n",
    "    try:\n",
    "        response = requests.get(uri)\n",
    "        response.raise_for_status()\n",
    "        # Check the file type (extension) and convert to JPG if needed\n",
    "        content_type = response.headers['Content-Type']\n",
    "        if content_type.startswith('image/'):\n",
    "            extension = content_type.split('/')[1]\n",
    "            if extension.lower() not in ('jpg', 'jpeg'):\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img.save(new_image_path, 'JPEG', quality=compression_quality)\n",
    "                print(f\"Converted and saved {uri} as JPG: {new_image_path}\", end='\\r')\n",
    "                download_ok = True\n",
    "            else:\n",
    "                with open(new_image_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                with Image.open(new_image_path) as img:\n",
    "                    # Save the image with the desired compression quality\n",
    "                    img.save(new_image_path, format='JPEG', quality=compression_quality)\n",
    "                print(f\"Downloaded and saved {uri} as JPG: {new_image_path}\", end='\\r')\n",
    "                download_ok = True\n",
    "        else:\n",
    "            print(f\"Skipping {uri} - Not an image\", end='\\r')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {uri}: {str(e)}\", end='\\r')\n",
    "        download_ok = False\n",
    "    if not download_ok:\n",
    "        return None\n",
    "    return {\"image\": new_image_path, \"caption\": caption}\n",
    "\n",
    "\n",
    "def download_and_rename(row, folder):\n",
    "    \"\"\"Downloads an image file and saves with the media item UUID as the filename\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    uri = row['image_file__uri']\n",
    "    # Check if uri exists and is a string\n",
    "    if uri and isinstance(uri, str):\n",
    "        uuid = row['media__uuid']\n",
    "        caption = row['caption']\n",
    "        return download_image_convert_to_jpg(uri, folder, uuid, caption)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Writing to 'jsonl' files\n",
    "def write_to_jsonl(new_data, jsonl_file):\n",
    "    \"\"\"Makes JSONL file with new_data\"\"\"\n",
    "    with open(jsonl_file, 'w') as file:\n",
    "        for json_dict in new_data:\n",
    "            line = json.dumps(json_dict)\n",
    "            file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a2b3b8-f54a-4866-aa8b-2095694c8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root_path for this jupyter notebook repo.\n",
    "repo_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "\n",
    "data_path = os.path.join(repo_path, 'json_data', 'artifact_images_w_sentence_captions.json')\n",
    "df = pd.read_json(data_path)\n",
    "\n",
    "# Change these as desired\n",
    "TRAIN_SIZE = 40000\n",
    "TEST_SIZE = 3500\n",
    "\n",
    "train_data_file = os.path.join(repo_path, 'files', 'train.json')\n",
    "test_data_file = os.path.join(repo_path, 'files', 'test.json')\n",
    "\n",
    "# If we don't have a train_data_file or a test data file, go out and make them!\n",
    "if not os.path.exists(train_data_file) or not os.path.exists(test_data_file):\n",
    "    # Separate out a training dataframe (train_df), a test dataframe (test_df)\n",
    "    train_df, rem_df = train_test_split(df, train_size=TRAIN_SIZE, random_state=42)\n",
    "    test_df = rem_df.sample(TEST_SIZE, random_state=42)\n",
    "    \n",
    "    train_files = [os.path.join(repo_path, 'files', 'training'),]\n",
    "    test_files = [os.path.join(repo_path, 'files', 'testing'),]\n",
    "    \n",
    "    train_data_file = os.path.join(repo_path, 'files', 'train.json')\n",
    "    test_data_file = os.path.join(repo_path, 'files', 'test.json')\n",
    "    \n",
    "    # Process train and test data\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        train_data = list(executor.map(download_and_rename, [row for _, row in train_df.iterrows()], train_files*len(train_df)))\n",
    "        test_data = list(executor.map(download_and_rename, [row for _, row in test_df.iterrows()], test_files*len(test_df)))\n",
    "    \n",
    "    write_to_jsonl(train_data, train_data_file)\n",
    "    write_to_jsonl(test_data, test_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f506e23-39cd-4785-94dc-4d948919d0a0",
   "metadata": {},
   "source": [
    "Now that we have the training and testing data files and the image files, let's train the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a154cfe6-b7e4-43b7-813b-50cdd653ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: datasets in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (2.14.5)\n",
      "Requirement already satisfied: Pillow in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: numpy in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torchvision) (1.26.1)\n",
      "Requirement already satisfied: requests in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchvision) (12.2.140)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->torchvision) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.2.140)\n",
      "Requirement already satisfied: requests in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ekansa/github/archaeology-images-ai/.venv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision datasets Pillow\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2888efeb-87ed-4432-9cb2-493ae58c094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first image: /home/ekansa/github/archaeology-images-ai/files/training/d98571c2-db9f-4dbe-883e-64d7e601aae6.jpg, caption: 'An image of an archaeological artifact found at Poggio Civitate, a place in Italy, within the Europe world region. This object came from a context dating to around 700 BCE to 535 BCE so it was probably made then or earlier. It has a general classification of inlay;  inlays (decorations) and mainly consists of ivory;  ivory (material). More descriptions include: Fragment Noted: false Fabric Category: Ivory Motif: Lotus Object Type: Ornament::Inlay / Inset Decorative Technique: Incised'\n"
     ]
    }
   ],
   "source": [
    "# test loading it back in\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=train_data_file)\n",
    "print(f\"first image: {dataset['train'][0]['image']}, caption: '{dataset['train'][0]['caption']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a11702f-9bb3-4d4c-a7af-7ee52bddbf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'finetune-clip-huggingface' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/damian0815/finetune-clip-huggingface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77cc460e-93c9-4d50-b72a-172e0a6987ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id =  \"openai/clip-vit-base-patch32\" # this was the clip version for stable diffusion 1.5\n",
    "#repo_id = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\" # this was the clip version for stable diffusion 2.0 onwards\n",
    "# however, using it requires more memory than I have available. More than what's available free tier google colab too.\n",
    "\n",
    "result_output_folder = os.path.join(repo_path, 'results')\n",
    "\n",
    "batch_size = 64\n",
    "num_train_epochs = 25\n",
    "max_token_seq_length = 77 # probably should be 100\n",
    "# NOTE ON learning_rate = \"1e-4\" # the prior parameter was \"5e-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab05e232-e261-44d9-84d6-fe79ac70a290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning openai/clip-vit-base-patch32 for 25 epochs with batch size 64, and then saving output to /home/ekansa/github/archaeology-images-ai/results.\n",
      "python finetune-clip-huggingface/huggingface_finetune_clip.py     --output_dir /home/ekansa/github/archaeology-images-ai/results     --model_name_or_path openai/clip-vit-base-patch32     --train_file /home/ekansa/github/archaeology-images-ai/files/train.json     --validation_file /home/ekansa/github/archaeology-images-ai/files/test.json     --image_column image     --overwrite_output_dir=True     --max_seq_length=77     --num_train_epochs=25     --caption_column caption     --overwrite_cache=True     --remove_unused_columns=False     --do_train     --per_device_train_batch_size=64     --per_device_eval_batch_size=64     --learning_rate=\"5e-5\" --warmup_steps=\"1\" --weight_decay 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Finetuning {repo_id} for {num_train_epochs} epochs with batch size {batch_size}, and then saving output to {result_output_folder}.\")\n",
    "\n",
    "print(f\"\"\"python finetune-clip-huggingface/huggingface_finetune_clip.py \\\n",
    "    --output_dir {result_output_folder} \\\n",
    "    --model_name_or_path {repo_id} \\\n",
    "    --train_file {train_data_file} \\\n",
    "    --validation_file {test_data_file} \\\n",
    "    --image_column image \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --max_seq_length={max_token_seq_length} \\\n",
    "    --num_train_epochs={num_train_epochs} \\\n",
    "    --caption_column caption \\\n",
    "    --overwrite_cache=True \\\n",
    "    --remove_unused_columns=False \\\n",
    "    --do_train \\\n",
    "    --per_device_train_batch_size={batch_size} \\\n",
    "    --per_device_eval_batch_size={batch_size} \\\n",
    "    --learning_rate=\"5e-5\" --warmup_steps=\"1\" --weight_decay 0.1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523a64d-2f24-461d-b5ab-50f9a4426324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2023 09:00:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "Running tokenizer on train dataset: 100%|█| 39716/39716 [00:03<00:00, 12949.80 e\n",
      "Parameter 'transform'=<function main.<locals>.<lambda> at 0x7f8254666020> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "11/02/2023 09:00:45 - WARNING - datasets.fingerprint - Parameter 'transform'=<function main.<locals>.<lambda> at 0x7f8254666020> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  2%|▋                                   | 293/15525 [15:30<18:07:48,  4.28s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "!python -W ignore finetune-clip-huggingface/huggingface_finetune_clip.py \\\n",
    "    --output_dir {result_output_folder} \\\n",
    "    --model_name_or_path {repo_id} \\\n",
    "    --train_file {train_data_file} \\\n",
    "    --validation_file {test_data_file} \\\n",
    "    --image_column image \\\n",
    "    --overwrite_output_dir=True \\\n",
    "    --max_seq_length={max_token_seq_length} \\\n",
    "    --num_train_epochs={num_train_epochs} \\\n",
    "    --caption_column caption \\\n",
    "    --overwrite_cache=True \\\n",
    "    --remove_unused_columns=False \\\n",
    "    --do_train \\\n",
    "    --per_device_train_batch_size={batch_size} \\\n",
    "    --per_device_eval_batch_size={batch_size} \\\n",
    "    --learning_rate=\"5e-5\" --warmup_steps=\"1\" --weight_decay 0.1\n",
    "print(\"--\\nDONE\")\n",
    "print(f\"If it worked, trained data should be in {result_output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bef16-28bd-4825-8a99-9fcc3d7d1b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193aabc-d1d8-40af-94f6-5a69b7899625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
