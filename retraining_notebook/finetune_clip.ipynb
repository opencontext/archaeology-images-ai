{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgMQePMfvCWh"
   },
   "outputs": [],
   "source": [
    "!unzip training.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnpEsjCSvJ7z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3Hvx5you1Kw"
   },
   "outputs": [],
   "source": [
    "#from https://github.com/shashnkvats/Indofashionclip/blob/main/indofashion_clip.py\n",
    "#with modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fHlk4sT-u3D3"
   },
   "outputs": [],
   "source": [
    "# the json file with filenames and captions\n",
    "json_path = 'simple.json'\n",
    "image_path = 'training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npX-LXi0BfHP"
   },
   "source": [
    "## The main event\n",
    "\n",
    "The code below should create something that is compatible with huggingface transformers architecture, which means it should be easier to integrate with llm-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jb6oAV79t0o",
    "outputId": "07fbc2e7-e761-4181-f4c6-9a36347ae6b1"
   },
   "outputs": [],
   "source": [
    "## try two\n",
    "## try two\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Setting cudnn benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    input_data = json.load(f)\n",
    "\n",
    "# Load CLIP model and processor from Hugging Face\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Loading model\n",
    "#single_gpu_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#model = torch.nn.DataParallel(single_gpu_model).to(device)   # Make model parallel\n",
    "#processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('mps')\n",
    "# Loading model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define a custom dataset\n",
    "\n",
    "from my_datasets import image_title_dataset\n",
    "\n",
    "def clean_caption(raw_caption, max_caption_char_limit=300):\n",
    "    if not raw_caption:\n",
    "       return ''\n",
    "    # just to be sure we're dealing with a string\n",
    "    raw_caption = str(raw_caption)\n",
    "    # Remove line breaks\n",
    "    raw_caption = raw_caption.replace('\\n', ' ')\n",
    "    # Remove all other punctuation.\n",
    "    raw_caption =  re.sub(r'[' + string.punctuation + r']+', ' ', raw_caption).strip()\n",
    "    ok_words = []\n",
    "    for act_word in raw_caption.split(' '):\n",
    "        if not act_word:\n",
    "           # Skip empty characters\n",
    "           continue\n",
    "        if len(' '.join(ok_words + [act_word])) < max_caption_char_limit:\n",
    "            # Adding the act_word is ok, because it's less than our\n",
    "            # character limits\n",
    "            ok_words.append(act_word)\n",
    "    # Combine it all into a single string.\n",
    "    caption = ' '.join(ok_words)\n",
    "    # truncating shouldn't be needed, but seems safer\n",
    "    return caption[:max_caption_char_limit]\n",
    "\n",
    "# Make sure each image path has one text\n",
    "list_image_path = []\n",
    "list_txt = []\n",
    "for item in input_data:\n",
    "  if 'filename' in item and 'captions' in item:\n",
    "    img_path = os.path.join('training', item['filename'].split('/')[-1])\n",
    "    # cleanup the text string of the caption(s)\n",
    "    caption = clean_caption(item['captions'])\n",
    "    # appending path to image then the corresponding caption\n",
    "    list_image_path.append(img_path)\n",
    "    list_txt.append(caption)\n",
    "\n",
    "dataset = image_title_dataset(list_image_path, list_txt)\n",
    "train_dataloader = DataLoader(dataset, batch_size=250, shuffle=True, num_workers=2) #Define your own dataloader\n",
    "\n",
    "# Gradient accumulation steps\n",
    "#accumulation_steps = 2\n",
    "\n",
    "# Adjust learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9,0.98), eps=1e-6, weight_decay=0.2)\n",
    "\n",
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total = len(train_dataloader))\n",
    "    for i, batch in enumerate(pbar):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        images,texts = batch\n",
    "        inputs = processor(texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "        # Normalize loss\n",
    "       #loss = loss / accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step and zero the gradients every accumulation_steps\n",
    "     #   if (i+1) % accumulation_steps == 0:\n",
    "     #       optimizer.step()\n",
    "      #      optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "        model.save_pretrained(f'./model_after_epoch_{epoch}')\n",
    "        #torch.save(model.module.state_dict(), f'./model_after_epoch_{epoch}')\n",
    "    # If the number of batches is not exactly divisible by accumulation_steps,\n",
    "    # make sure to still zero the gradients after finishing an epoch\n",
    "    if len(train_dataloader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what I got when I went with optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9,0.98), eps=1e-6), weight decay turned off \n",
    "\n",
    "Loss: 4.8167: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [14:48<00:00, 42.29s/it]\n",
    "Epoch 1/10, Loss: 4.5752: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [14:18<00:00, 40.86s/it]\n",
    "Epoch 2/10, Loss: 4.8087: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [14:15<00:00, 40.75s/it]\n",
    "Epoch 3/10, Loss: 4.6696: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [15:17<00:00, 43.71s/it]\n",
    "Epoch 4/10, Loss: 4.5540: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [15:16<00:00, 43.65s/it]\n",
    "Epoch 5/10, Loss: 4.5737: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [15:11<00:00, 43.41s/it]\n",
    "Epoch 6/10, Loss: 4.5502: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [15:00<00:00, 42.89s/it]\n",
    "Epoch 7/10, Loss: 4.5466: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [14:54<00:00, 42.58s/it]\n",
    "Epoch 8/10, Loss: 4.5443: 100%|█████████████████████████████████████████████████████████████████████| 21/21 [15:29<00:00, 44.25s/it]\n",
    "Epoch 9/10, Loss: 4.5446:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/3, Loss: 3.7844: 100%|████████████████████████████████████████████████████████████████████| 102/102 [10:00<00:00,  5.88s/it]\n",
      "Epoch 1/3, Loss: 3.7843: 100%|████████████████████████████████████████████████████████████████████| 102/102 [09:56<00:00,  5.85s/it]\n",
      "Epoch 2/3, Loss: 3.7851: 100%|████████████████████████████████████████████████████████████████████| 102/102 [09:40<00:00,  5.69s/it]\n"
     ]
    }
   ],
   "source": [
    "## trying to implement a learning rate scheduler\n",
    "## yes, gpt4 suggested this: 1. **Learning Rate Adjustment**: \n",
    "#You might want to experiment with a learning rate scheduler for adjusting the learning rate over the epochs. \n",
    "#This could help the model learn better and not get stuck giving the same results. \n",
    "#Adam optimizer often works best with learning rate decay.\n",
    "\n",
    "## try three\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Setting cudnn benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    input_data = json.load(f)\n",
    "\n",
    "# Load CLIP model and processor from Hugging Face\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Loading model\n",
    "#single_gpu_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#model = torch.nn.DataParallel(single_gpu_model).to(device)   # Make model parallel\n",
    "#processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('mps')\n",
    "# Loading model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define a custom dataset\n",
    "\n",
    "from my_datasets import image_title_dataset\n",
    "\n",
    "def clean_caption(raw_caption, max_caption_char_limit=300):\n",
    "    if not raw_caption:\n",
    "       return ''\n",
    "    # just to be sure we're dealing with a string\n",
    "    raw_caption = str(raw_caption)\n",
    "    # Remove line breaks\n",
    "    raw_caption = raw_caption.replace('\\n', ' ')\n",
    "    # Remove all other punctuation.\n",
    "    raw_caption =  re.sub(r'[' + string.punctuation + r']+', ' ', raw_caption).strip()\n",
    "    ok_words = []\n",
    "    for act_word in raw_caption.split(' '):\n",
    "        if not act_word:\n",
    "           # Skip empty characters\n",
    "           continue\n",
    "        if len(' '.join(ok_words + [act_word])) < max_caption_char_limit:\n",
    "            # Adding the act_word is ok, because it's less than our\n",
    "            # character limits\n",
    "            ok_words.append(act_word)\n",
    "    # Combine it all into a single string.\n",
    "    caption = ' '.join(ok_words)\n",
    "    # truncating shouldn't be needed, but seems safer\n",
    "    return caption[:max_caption_char_limit]\n",
    "\n",
    "# Make sure each image path has one text\n",
    "list_image_path = []\n",
    "list_txt = []\n",
    "for item in input_data:\n",
    "  if 'filename' in item and 'captions' in item:\n",
    "    img_path = os.path.join('training', item['filename'].split('/')[-1])\n",
    "    # cleanup the text string of the caption(s)\n",
    "    caption = clean_caption(item['captions'])\n",
    "    # appending path to image then the corresponding caption\n",
    "    list_image_path.append(img_path)\n",
    "    list_txt.append(caption)\n",
    "\n",
    "dataset = image_title_dataset(list_image_path, list_txt)\n",
    "train_dataloader = DataLoader(dataset, batch_size=50, shuffle=True, num_workers=2) #Define your own dataloader\n",
    "\n",
    "# Gradient accumulation steps\n",
    "#accumulation_steps = 2\n",
    "\n",
    "# adjust learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.1)\n",
    "\n",
    "# use a StepLR scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total = len(train_dataloader))\n",
    "    for i, batch in enumerate(pbar):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        images,texts = batch\n",
    "        inputs = processor(texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # making sure the optimizer steps and then the gradients are zeroed in each loop\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "        model.save_pretrained(f'./model_after_epoch_{epoch}')\n",
    "    # If the number of batches is not exactly divisible by accumulation_steps,\n",
    "    # make sure to still zero the gradients after finishing an epoch\n",
    "    if len(train_dataloader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
