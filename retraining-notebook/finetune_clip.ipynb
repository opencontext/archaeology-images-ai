{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgMQePMfvCWh"
   },
   "outputs": [],
   "source": [
    "!unzip training.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PnpEsjCSvJ7z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: requests in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torchvision) (2.29.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: filelock in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shawngraham/mambaforge/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.15.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3Hvx5you1Kw"
   },
   "outputs": [],
   "source": [
    "#from https://github.com/shashnkvats/Indofashionclip/blob/main/indofashion_clip.py\n",
    "#with modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fHlk4sT-u3D3"
   },
   "outputs": [],
   "source": [
    "# the json file with filenames and captions\n",
    "json_path = 'simple.json'\n",
    "image_path = 'training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npX-LXi0BfHP"
   },
   "source": [
    "## The main event\n",
    "\n",
    "The code below should create something that is compatible with huggingface transformers architecture, which means it should be easier to integrate with llm-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jb6oAV79t0o",
    "outputId": "07fbc2e7-e761-4181-f4c6-9a36347ae6b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 0/15 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Epoch 0/30, Loss: 1.6844: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:51<00:00,  7.43s/it]\n",
      "Epoch 1/30, Loss: 1.6837: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:48<00:00,  7.24s/it]\n",
      "Epoch 2/30, Loss: 1.6837: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:47<00:00,  7.18s/it]\n",
      "Epoch 3/30, Loss: 1.6837: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:45<00:00,  7.06s/it]\n",
      "  0%|                                                                                                                              | 0/15 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Normalize loss\u001b[39;00m\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[0;32m---> 90\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Optimizer step and zero the gradients every accumulation_steps\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## try two\n",
    "from PIL import Image\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "\n",
    "# Setting cudnn benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    input_data = json.load(f)\n",
    "\n",
    "# Load CLIP model and processor from Hugging Face\n",
    "\n",
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Loading model\n",
    "single_gpu_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = torch.nn.DataParallel(single_gpu_model).to(device)   # Make model parallel\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "        self.image_path = list_image_path\n",
    "        self.list_txt = list_txt\n",
    "        self.transform = Compose([Resize((224, 224)), ToTensor()])  # Resize all images to 224x224\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_txt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      try:\n",
    "        image = Image.open(self.image_path[idx]).convert(\"RGB\")\n",
    "        image = self.transform(image)  # Apply the transforms\n",
    "      except Exception as e:\n",
    "        print(f\"Unable to open image at {self.image_path[idx]} due to error : {e}\")\n",
    "        image = torch.zeros(3, 224, 224)  # Adding a Zero tensor in case of image loading failure\n",
    "        # Return an empty string as text\n",
    "      return image, self.list_txt[idx] if self.list_txt[idx] else [\"\"]\n",
    "# Make sure each image path has one text\n",
    "list_image_path = []\n",
    "list_txt = []\n",
    "for item in input_data:\n",
    "  if 'filename' in item and 'captions' in item:\n",
    "    img_path = os.path.join('training', item['filename'].split('/')[-1])\n",
    "    caption = item['captions'][:40]\n",
    "    # appending path to image then the corresponding caption\n",
    "    list_image_path.append(img_path)\n",
    "    list_txt.append(caption)\n",
    "\n",
    "dataset = image_title_dataset(list_image_path, list_txt)\n",
    "# using this notebook locally, numworkers has to be set to 0\n",
    "train_dataloader = DataLoader(dataset, batch_size=100, shuffle=True, num_workers=0) #Define your own dataloader\n",
    "\n",
    "# Gradient accumulation steps\n",
    "accumulation_steps = 2\n",
    "\n",
    "# Adjust learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5 * accumulation_steps)\n",
    "\n",
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total = len(train_dataloader))\n",
    "    for i, batch in enumerate(pbar):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        images,texts = batch\n",
    "        inputs = processor(texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "        # Normalize loss\n",
    "        loss = loss / accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step and zero the gradients every accumulation_steps\n",
    "        if (i+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "       # model.save_pretrained(f'./model_after_epoch_{epoch}')\n",
    "        torch.save(model.module.state_dict(), f'./model_after_epoch_{epoch}')\n",
    "    # If the number of batches is not exactly divisible by accumulation_steps,\n",
    "    # make sure to still zero the gradients after finishing an epoch\n",
    "    if len(train_dataloader) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhYaSn06yJ0r"
   },
   "outputs": [],
   "source": [
    "## and now, we do a little sleight of hand to get the resulting model\n",
    "## reshaped for how we want to use it\n",
    "## this is because the dataParallel method can't use save_pretrained.\n",
    "\n",
    "# change X below:\n",
    "# Load weights into the model for other tasks\n",
    "base_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "base_model.load_state_dict(torch.load('model_after_epoch_X'))\n",
    "\n",
    "base_model.save_pretrained(f'./archae_ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O19iRimACjhh",
    "outputId": "4b8eaa34-06d9-465d-b1e3-e55c422be1f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: model_after_epoch_0/ (stored 0%)\n",
      "  adding: model_after_epoch_0/pytorch_model.bin (deflated 7%)\n",
      "  adding: model_after_epoch_0/config.json (deflated 46%)\n"
     ]
    }
   ],
   "source": [
    "!zip test.zip -r archae_ai/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
